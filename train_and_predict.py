import os
import pandas as pd
import numpy as np
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.model_selection import cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report
from scipy.sparse import hstack
import warnings
import xgboost as xgb
warnings.filterwarnings('ignore')

# === ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–¢é€£ã®é–¢æ•° ===
def read_text_pair(folder, article_index):
    """æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰file_1.txtã¨file_2.txtã‚’èª­ã¿è¾¼ã‚€é–¢æ•°"""
    folder_name = f"article_{int(article_index):04d}"
    base_path = os.path.join(folder, folder_name)
    with open(os.path.join(base_path, "file_1.txt"), encoding="utf-8") as f:
        file1 = f.read()
    with open(os.path.join(base_path, "file_2.txt"), encoding="utf-8") as f:
        file2 = f.read()
    return file1, file2

def extract_advanced_features(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ‹¡å¼µã•ã‚ŒãŸçµ±è¨ˆçš„ç‰¹å¾´é‡ã‚’æŠ½å‡ºã™ã‚‹é–¢æ•°"""
    text = text.strip()
    if not text:
        return [0.0] * 16  # å›ºå®šã•ã‚ŒãŸç‰¹å¾´é‡æ•°
    
    # åŸºæœ¬çµ±è¨ˆ
    words = text.split()
    word_count = len(words)
    char_count = len(text)
    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]
    sentence_count = len(sentences)
    
    # åŸºæœ¬ç‰¹å¾´é‡
    avg_word_length = np.mean([len(word) for word in words]) if words else 0.0
    avg_sentence_length = word_count / sentence_count if sentence_count > 0 else 0.0
    
    # èªå½™ã®å¤šæ§˜æ€§
    if word_count > 0:
        unique_words = len(set(word.lower() for word in words))
        lexical_diversity = unique_words / word_count
    else:
        unique_words = 0
        lexical_diversity = 0.0
    
    # å¥èª­ç‚¹ã®ç‰¹å¾´é‡
    punctuation_density = len(re.findall(r'[.!?,:;]', text)) / char_count if char_count > 0 else 0.0
    exclamation_count = text.count('!')
    question_count = text.count('?')
    
    # æ–°è¦è¿½åŠ ç‰¹å¾´é‡
    # 1. ã‚¨ãƒ©ãƒ¼æ¤œå‡ºç‰¹å¾´é‡
    spelling_errors = detect_spelling_errors(text)
    grammar_errors = detect_grammar_errors(text)
    
    # 2. å°‚é–€ç”¨èªå¯†åº¦
    technical_density = calculate_technical_density(text)
    
    # 3. æ„Ÿæƒ…åˆ†æç‰¹å¾´é‡
    sentiment_score, subjectivity_score = analyze_sentiment(text)
    
    # 4. æ–‡æ§‹é€ ã®è¤‡é›‘æ€§
    complexity_score = calculate_complexity(text)
    
    # 5. å¤§æ–‡å­—ä½¿ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³
    uppercase_ratio = sum(1 for c in text if c.isupper()) / char_count if char_count > 0 else 0.0
    
    # å…¨ã¦ã®ç‰¹å¾´é‡ã‚’æµ®å‹•å°æ•°ç‚¹æ•°ã«å¤‰æ›ã—ã€NaN/Infã‚’0ã«ç½®æ›
    features = [
        float(word_count), float(char_count), float(sentence_count),
        float(avg_word_length), float(avg_sentence_length),
        float(lexical_diversity), float(punctuation_density),
        float(exclamation_count), float(question_count),
        float(spelling_errors), float(grammar_errors),
        float(technical_density), float(sentiment_score), float(subjectivity_score),
        float(complexity_score), float(uppercase_ratio)
    ]
    
    # NaN/Infã‚’0ã«ç½®æ›
    features = [0.0 if np.isnan(f) or np.isinf(f) else f for f in features]
    
    return features

def detect_spelling_errors(text):
    """ç°¡æ˜“çš„ãªã‚¹ãƒšãƒ«ãƒŸã‚¹æ¤œå‡ºï¼ˆè‹±èªã®ä¸€èˆ¬çš„ãªé–“é•ã„ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰"""
    # ä¸€èˆ¬çš„ãªã‚¹ãƒšãƒ«ãƒŸã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³
    error_patterns = [
        r'\b\w*off\s+information\b',  # "off information" instead of "of information"
        r'\btypescombinations\b',      # "typescombinations" (å˜èªã®çµåˆãƒŸã‚¹)
        r'\ballthe\b',                 # "allthe" instead of "all the"
        r'\bwhich\s+which\b',          # é‡è¤‡
        r'\bthe\s+the\b',              # é‡è¤‡
        r'\band\s+and\b',              # é‡è¤‡
    ]
    
    error_count = 0
    for pattern in error_patterns:
        error_count += len(re.findall(pattern, text, re.IGNORECASE))
    
    return error_count

def detect_grammar_errors(text):
    """ç°¡æ˜“çš„ãªæ–‡æ³•ã‚¨ãƒ©ãƒ¼æ¤œå‡º"""
    # æ–‡æ³•ã‚¨ãƒ©ãƒ¼ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
    grammar_patterns = [
        r'\ba\s+[aeiou]',              # "a" before vowel sound
        r'\ban\s+[bcdfghjklmnpqrstvwxyz]',  # "an" before consonant sound
        r'\s{2,}',                     # è¤‡æ•°ã®é€£ç¶šã‚¹ãƒšãƒ¼ã‚¹
        r'[a-z][A-Z]',                 # å¤§æ–‡å­—å°æ–‡å­—ã®ä¸é©åˆ‡ãªé…ç½®
        r'[.!?]\s*[a-z]',              # æ–‡ã®å§‹ã¾ã‚ŠãŒå°æ–‡å­—
    ]
    
    error_count = 0
    for pattern in grammar_patterns:
        error_count += len(re.findall(pattern, text))
    
    return error_count

def calculate_technical_density(text):
    """æŠ€è¡“ãƒ»å°‚é–€ç”¨èªã®å¯†åº¦ã‚’è¨ˆç®—"""
    # å¤©æ–‡å­¦ãƒ»ç§‘å­¦æŠ€è¡“ç”¨èª
    technical_terms = [
        'telescope', 'astronomical', 'photometric', 'infrared', 'survey',
        'observatory', 'celestial', 'spacecraft', 'dataset', 'calibrated',
        'wavelength', 'spectrum', 'magnitude', 'luminosity', 'parallax',
        'redshift', 'nebula', 'galaxy', 'stellar', 'cosmic', 'exoplanet',
        'petabyte', 'terabyte', 'database', 'algorithm', 'processing'
    ]
    
    text_lower = text.lower()
    count = sum(1 for term in technical_terms if term in text_lower)
    word_count = len(text.split())
    
    return count / word_count if word_count > 0 else 0.0

def analyze_sentiment(text):
    """ç°¡æ˜“çš„ãªæ„Ÿæƒ…åˆ†æï¼ˆè¾æ›¸ãƒ™ãƒ¼ã‚¹ï¼‰"""
    # ãƒã‚¸ãƒ†ã‚£ãƒ–å˜èª
    positive_words = ['excellent', 'great', 'amazing', 'wonderful', 'fantastic', 
                     'impressive', 'significant', 'valuable', 'important', 'useful']
    
    # ãƒã‚¬ãƒ†ã‚£ãƒ–å˜èª
    negative_words = ['bad', 'poor', 'terrible', 'awful', 'horrible', 
                     'difficult', 'problem', 'error', 'failure', 'wrong']
    
    # ä¸»è¦³çš„å˜èª
    subjective_words = ['believe', 'think', 'feel', 'hope', 'wish', 
                       'probably', 'maybe', 'perhaps', 'seems', 'appears']
    
    text_lower = text.lower()
    words = text_lower.split()
    
    pos_count = sum(1 for word in words if word in positive_words)
    neg_count = sum(1 for word in words if word in negative_words)
    subj_count = sum(1 for word in words if word in subjective_words)
    
    sentiment_score = (pos_count - neg_count) / len(words) if words else 0.0
    subjectivity_score = subj_count / len(words) if words else 0.0
    
    return sentiment_score, subjectivity_score

def calculate_complexity(text):
    """æ–‡ã®è¤‡é›‘æ€§ã‚’è¨ˆç®—"""
    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]
    if not sentences:
        return 0.0
    
    # æ¥ç¶šè©ã®ä½¿ç”¨é »åº¦
    connectives = ['however', 'therefore', 'moreover', 'furthermore', 
                  'additionally', 'consequently', 'nevertheless', 'although']
    
    complexity_score = 0.0
    for sentence in sentences:
        # æ–‡ã®é•·ã•ã«ã‚ˆã‚‹è¤‡é›‘æ€§
        complexity_score += len(sentence.split()) / 10.0
        
        # æ¥ç¶šè©ã®ä½¿ç”¨
        for conn in connectives:
            if conn in sentence.lower():
                complexity_score += 1.0
    
    return complexity_score / len(sentences)

def create_enhanced_features(file1, file2):
    """2ã¤ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ‹¡å¼µã•ã‚ŒãŸç‰¹å¾´é‡ã‚’ä½œæˆã™ã‚‹é–¢æ•°"""
    # TF-IDFç”¨ã®ãƒ†ã‚­ã‚¹ãƒˆ
    combined_text = file1 + " [SEP] " + file2
    
    # æ‹¡å¼µã•ã‚ŒãŸçµ±è¨ˆçš„ç‰¹å¾´é‡
    features1 = extract_advanced_features(file1)
    features2 = extract_advanced_features(file2)
    
    # ç‰¹å¾´é‡ã®æ¬¡å…ƒæ•°ã‚’ç¢ºèª
    assert len(features1) == 16, f"features1ã®æ¬¡å…ƒæ•°ãŒé–“é•ã£ã¦ã„ã¾ã™: {len(features1)}"
    assert len(features2) == 16, f"features2ã®æ¬¡å…ƒæ•°ãŒé–“é•ã£ã¦ã„ã¾ã™: {len(features2)}"
    
    # å·®åˆ†ç‰¹å¾´é‡
    diff_features = [f1 - f2 for f1, f2 in zip(features1, features2)]
    
    # æ¯”ç‡ç‰¹å¾´é‡
    ratio_features = []
    for f1, f2 in zip(features1, features2):
        if f2 != 0:
            ratio = f1 / f2
        else:
            ratio = 0.0 if f1 == 0 else 1.0
        ratio_features.append(ratio)
    
    # è¿½åŠ ã®æ¯”è¼ƒç‰¹å¾´é‡
    # èªå½™ã®é‡è¤‡åº¦
    words1 = set(file1.lower().split())
    words2 = set(file2.lower().split())
    vocabulary_overlap = len(words1 & words2) / len(words1 | words2) if words1 | words2 else 0.0
    
    # æ–‡é•·ã®åˆ†æ•£
    sentences1 = [s.strip() for s in re.split(r'[.!?]+', file1) if s.strip()]
    sentences2 = [s.strip() for s in re.split(r'[.!?]+', file2) if s.strip()]
    
    var1 = np.var([len(s.split()) for s in sentences1]) if sentences1 else 0.0
    var2 = np.var([len(s.split()) for s in sentences2]) if sentences2 else 0.0
    sentence_length_var_diff = var1 - var2
    
    # å…¨ç‰¹å¾´é‡ã‚’çµåˆ (16 + 16 + 16 + 16 + 2 = 66æ¬¡å…ƒ)
    all_features = features1 + features2 + diff_features + ratio_features + [vocabulary_overlap, sentence_length_var_diff]
    
    # NaN/Infã‚’0ã«ç½®æ›
    all_features = [0.0 if np.isnan(f) or np.isinf(f) else float(f) for f in all_features]
    
    # æœ€çµ‚çš„ãªæ¬¡å…ƒæ•°ã‚’ç¢ºèª
    assert len(all_features) == 66, f"æœ€çµ‚ç‰¹å¾´é‡ã®æ¬¡å…ƒæ•°ãŒé–“é•ã£ã¦ã„ã¾ã™: {len(all_features)}"
    
    return combined_text, all_features

# === ç‰¹å¾´é‡ä½œæˆé–¢é€£ã®é–¢æ•° ===
def prepare_training_data(train_df):
    """è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç‰¹å¾´é‡ã¨ãƒ©ãƒ™ãƒ«ã‚’ä½œæˆã™ã‚‹é–¢æ•°"""
    X_text = []
    X_features = []
    y = []
    
    print("\nå„è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ‹¡å¼µç‰¹å¾´é‡ã‚’æŠ½å‡ºä¸­...")
    for i, row in train_df.iterrows():
        try:
            file1, file2 = read_text_pair("train", row["id"])
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³1: file1 + [SEP] + file2
            combined_text1, features1 = create_enhanced_features(file1, file2)
            X_text.append(combined_text1)
            X_features.append(features1)
            y.append(1 if row["real_text_id"] == 1 else 0)
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³2: file2 + [SEP] + file1
            combined_text2, features2 = create_enhanced_features(file2, file1)
            X_text.append(combined_text2)
            X_features.append(features2)
            y.append(1 if row["real_text_id"] == 2 else 0)
            
        except Exception as e:
            print(f"ã‚¨ãƒ©ãƒ¼: ID {row['id']} ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            continue
    
    print(f"ğŸ“Š å­¦ç¿’ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(X_text)} (å…ƒãƒ‡ãƒ¼ã‚¿ã®2å€)")
    print(f"ğŸ“Š ãƒ©ãƒ™ãƒ«åˆ†å¸ƒ: {pd.Series(y).value_counts()}")
    
    # ç‰¹å¾´é‡ã®æ¬¡å…ƒæ•°ã‚’ç¢ºèª
    if X_features:
        print(f"ğŸ“Š ç‰¹å¾´é‡ã®æ¬¡å…ƒæ•°: {len(X_features[0])}")
        # å…¨ã¦ã®ç‰¹å¾´é‡ãŒåŒã˜æ¬¡å…ƒæ•°ã‹ãƒã‚§ãƒƒã‚¯
        feature_lengths = [len(f) for f in X_features]
        if not all(length == feature_lengths[0] for length in feature_lengths):
            print("âš ï¸  ç‰¹å¾´é‡ã®æ¬¡å…ƒæ•°ãŒä¸€è‡´ã—ã¦ã„ã¾ã›ã‚“!")
            for i, length in enumerate(feature_lengths):
                if length != feature_lengths[0]:
                    print(f"   ã‚µãƒ³ãƒ—ãƒ« {i}: {length} æ¬¡å…ƒ")
    
    return X_text, X_features, y

def create_optimized_tfidf_features(X_text):
    """æœ€é©åŒ–ã•ã‚ŒãŸTF-IDFç‰¹å¾´é‡ã‚’ä½œæˆã™ã‚‹é–¢æ•°"""
    print("\n=== æœ€é©åŒ–ã•ã‚ŒãŸTF-IDFç‰¹å¾´é‡ã®ä½œæˆ ===")
    
    # ã‚ˆã‚Šåºƒç¯„å›²ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§æœ€é©åŒ–
    vectorizer = TfidfVectorizer(
        max_features=3000,        # ç‰¹å¾´é‡æ•°ã‚’å¢—åŠ 
        min_df=2,
        max_df=0.85,             # é–¾å€¤ã‚’å¾®èª¿æ•´
        ngram_range=(1, 3),      # 3-gramã¾ã§æ‹¡å¼µ
        stop_words='english',
        lowercase=True,
        strip_accents='unicode',
        token_pattern=r'\b\w+\b'  # ã‚ˆã‚Šå³å¯†ãªãƒˆãƒ¼ã‚¯ãƒ³åŒ–
    )
    
    X_tfidf = vectorizer.fit_transform(X_text)
    print(f"ğŸ“Š TF-IDFç‰¹å¾´é‡ã®å½¢çŠ¶: {X_tfidf.shape}")
    
    return vectorizer, X_tfidf

def prepare_statistical_features(X_features):
    """çµ±è¨ˆçš„ç‰¹å¾´é‡ã‚’æº–å‚™ã™ã‚‹é–¢æ•°"""
    print("\n=== æ‹¡å¼µçµ±è¨ˆçš„ç‰¹å¾´é‡ã®æº–å‚™ ===")
    
    # ãƒªã‚¹ãƒˆã‚’NumPyé…åˆ—ã«å¤‰æ›å‰ã«ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¡¨ç¤º
    print(f"ğŸ“Š ç‰¹å¾´é‡ãƒªã‚¹ãƒˆã®é•·ã•: {len(X_features)}")
    if X_features:
        print(f"ğŸ“Š å„ç‰¹å¾´é‡ã®æ¬¡å…ƒæ•°: {[len(f) for f in X_features[:5]]}")  # æœ€åˆã®5ã¤ã‚’è¡¨ç¤º
    
    try:
        X_features_array = np.array(X_features, dtype=np.float64)
        print(f"ğŸ“Š å¤‰æ›å¾Œã®é…åˆ—å½¢çŠ¶: {X_features_array.shape}")
    except ValueError as e:
        print(f"âš ï¸  é…åˆ—å¤‰æ›ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
        # å„ç‰¹å¾´é‡ã®é•·ã•ã‚’ãƒã‚§ãƒƒã‚¯
        feature_lengths = [len(f) for f in X_features]
        unique_lengths = set(feature_lengths)
        print(f"âš ï¸  ç‰¹å¾´é‡ã®é•·ã•ã®ç¨®é¡: {unique_lengths}")
        
        # æœ€ã‚‚å¤šã„é•·ã•ã«åˆã‚ã›ã¦èª¿æ•´
        most_common_length = max(set(feature_lengths), key=feature_lengths.count)
        print(f"âš ï¸  æœ€ã‚‚å¤šã„é•·ã•: {most_common_length}")
        
        # é•·ã•ã‚’çµ±ä¸€
        X_features_fixed = []
        for f in X_features:
            if len(f) == most_common_length:
                X_features_fixed.append(f)
            elif len(f) < most_common_length:
                # ä¸è¶³åˆ†ã‚’0ã§åŸ‹ã‚ã‚‹
                X_features_fixed.append(f + [0.0] * (most_common_length - len(f)))
            else:
                # é•·ã™ãã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚ã‚‹
                X_features_fixed.append(f[:most_common_length])
        
        X_features_array = np.array(X_features_fixed, dtype=np.float64)
        print(f"ğŸ“Š ä¿®æ­£å¾Œã®é…åˆ—å½¢çŠ¶: {X_features_array.shape}")
    
    if np.any(np.isnan(X_features_array)):
        print("âš ï¸  NaNå€¤ã‚’æ¤œå‡ºã—ã¾ã—ãŸã€‚0ã«ç½®æ›ã—ã¾ã™ã€‚")
        X_features_array = np.nan_to_num(X_features_array, nan=0.0)
    
    if np.any(np.isinf(X_features_array)):
        print("âš ï¸  Infå€¤ã‚’æ¤œå‡ºã—ã¾ã—ãŸã€‚0ã«ç½®æ›ã—ã¾ã™ã€‚")
        X_features_array = np.nan_to_num(X_features_array, posinf=0.0, neginf=0.0)
    
    scaler = StandardScaler()
    X_features_scaled = scaler.fit_transform(X_features_array)
    print(f"âœ… æ‹¡å¼µçµ±è¨ˆçš„ç‰¹å¾´é‡ã‚’æ­£è¦åŒ–ã—ã¾ã—ãŸ (æ¬¡å…ƒæ•°: {X_features_scaled.shape[1]})")
    
    return scaler, X_features_scaled

def combine_features(X_tfidf, X_features_scaled):
    """ç‰¹å¾´é‡ã‚’çµåˆã™ã‚‹é–¢æ•°"""
    print("\n=== ç‰¹å¾´é‡ã®çµåˆ ===")
    X_combined = hstack([X_tfidf, X_features_scaled])
    
    if hasattr(X_combined, 'data') and np.any(np.isnan(X_combined.data)):
        print("âš ï¸  ã‚¹ãƒ‘ãƒ¼ã‚¹è¡Œåˆ—å†…ã®NaNå€¤ã‚’ä¿®æ­£ä¸­...")
        X_combined = X_combined.toarray()
        X_combined = np.nan_to_num(X_combined, nan=0.0)
    
    print(f"ğŸ“Š æœ€çµ‚çš„ãªç‰¹å¾´é‡è¡Œåˆ—: {X_combined.shape}")
    return X_combined

# === æ”¹è‰¯ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«é–¢é€£ã®é–¢æ•° ===
def create_ensemble_model():
    """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã™ã‚‹é–¢æ•°"""
    print("\n=== ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ ===")
    
    # ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«1: ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°
    lr_model = LogisticRegression(
        random_state=42,
        max_iter=3000,
        solver='liblinear',
        C=0.5,                    # æ­£å‰‡åŒ–ã‚’å¼·åŒ–
        penalty='l2'
    )
    
    # ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«2: ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ
    rf_model = RandomForestClassifier(
        n_estimators=100,
        max_depth=20,
        min_samples_split=5,
        min_samples_leaf=2,
        random_state=42,
        n_jobs=-1
    )

    xgb_model = xgb.XGBClassifier(
        n_estimators=100,
        max_depth=6,
        learning_rate=0.1,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42
    )
    
    # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«
    ensemble_model = VotingClassifier([
        ('lr', LogisticRegression()),
        ('rf', RandomForestClassifier()),
        ('xgb', xgb_model)
    ], voting='soft')
    
    return ensemble_model

def train_and_optimize_model(X_combined, y):
    """ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã—æœ€é©åŒ–ã™ã‚‹é–¢æ•°"""
    print("\n=== ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã¨æœ€é©åŒ– ===")
    
    # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ
    model = create_ensemble_model()
    
    # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æœ€é©åŒ–ï¼ˆç°¡æ˜“ç‰ˆï¼‰
    param_grid = {
        'lr__C': [0.1, 0.5, 1.0],
        'rf__n_estimators': [50, 100, 150],
        'rf__max_depth': [15, 20, 25]
    }
    
    print("ğŸ“Š ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã«ã‚ˆã‚‹æœ€é©åŒ–ã‚’å®Ÿè¡Œä¸­...")
    grid_search = GridSearchCV(
        model, 
        param_grid, 
        cv=3,  # è¨ˆç®—æ™‚é–“ã‚’è€ƒæ…®ã—ã¦3-fold
        scoring='accuracy',
        n_jobs=-1,
        verbose=1
    )
    
    grid_search.fit(X_combined, y)
    best_model = grid_search.best_estimator_
    
    print(f"âœ… æœ€é©ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {grid_search.best_params_}")
    print(f"âœ… æœ€é©åŒ–å¾Œã®ã‚¹ã‚³ã‚¢: {grid_search.best_score_:.4f}")
    
    return best_model

def evaluate_model(model, X_combined, y):
    """ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’è©•ä¾¡ã™ã‚‹é–¢æ•°"""
    print("\n=== äº¤å·®æ¤œè¨¼ã«ã‚ˆã‚‹æ€§èƒ½è©•ä¾¡ ===")
    cv_scores = cross_val_score(model, X_combined, y, cv=5, scoring='accuracy')
    print(f"ğŸ“Š äº¤å·®æ¤œè¨¼ã‚¹ã‚³ã‚¢: {cv_scores}")
    print(f"ğŸ“Š å¹³å‡ã‚¹ã‚³ã‚¢: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")
    
    return cv_scores

def predict_test_data_enhanced(test_dir, vectorizer, scaler, model):
    """æ”¹è‰¯ã•ã‚ŒãŸãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿äºˆæ¸¬é–¢æ•°"""
    print("\n=== ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®äºˆæ¸¬ï¼ˆæ”¹è‰¯ç‰ˆï¼‰ ===")
    test_folders = sorted([f for f in os.listdir(test_dir) if f.startswith("article_")])
    print(f"ğŸ“Š ãƒ†ã‚¹ãƒˆãƒ•ã‚©ãƒ«ãƒ€æ•°: {len(test_folders)}")
    
    results = []
    prediction_confidence = []
    
    for i, folder in enumerate(test_folders):
        try:
            article_id = folder.split("_")[1]
            file1, file2 = read_text_pair(test_dir, article_id)
            
            # è¤‡æ•°å›ã®äºˆæ¸¬ã§å®‰å®šæ€§ã‚’ç¢ºä¿
            prob1_list = []
            prob2_list = []
            
            for _ in range(3):  # 3å›ã®äºˆæ¸¬
                prob1 = predict_single_pair_enhanced(file1, file2, vectorizer, scaler, model)
                prob2 = predict_single_pair_enhanced(file2, file1, vectorizer, scaler, model)
                prob1_list.append(prob1)
                prob2_list.append(prob2)
            
            # å¹³å‡ã‚’å–ã‚‹
            prob1_avg = np.mean(prob1_list)
            prob2_avg = np.mean(prob2_list)
            
            # äºˆæ¸¬çµæœã¨ä¿¡é ¼åº¦
            real = 1 if prob1_avg > prob2_avg else 2
            confidence = abs(prob1_avg - prob2_avg)
            
            results.append((int(article_id), real))
            prediction_confidence.append(confidence)
            
            if i < 5:
                print(f"ãƒ†ã‚¹ãƒˆ {i}: file1ç¢ºç‡={prob1_avg:.4f}, file2ç¢ºç‡={prob2_avg:.4f}, äºˆæ¸¬={real}, ä¿¡é ¼åº¦={confidence:.4f}")
        
        except Exception as e:
            print(f"âš ï¸  ãƒ†ã‚¹ãƒˆ {folder} ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆäºˆæ¸¬ã‚’è¿½åŠ 
            article_id = folder.split("_")[1]
            results.append((int(article_id), 1))
            prediction_confidence.append(0.5)
    
    # ä¿¡é ¼åº¦ã®åˆ†æ
    print(f"\nğŸ“Š äºˆæ¸¬ä¿¡é ¼åº¦çµ±è¨ˆ:")
    print(f"   å¹³å‡ä¿¡é ¼åº¦: {np.mean(prediction_confidence):.4f}")
    print(f"   ä¿¡é ¼åº¦ä¸­å¤®å€¤: {np.median(prediction_confidence):.4f}")
    print(f"   ä½ä¿¡é ¼åº¦äºˆæ¸¬æ•° (<0.1): {sum(1 for c in prediction_confidence if c < 0.1)}")
    
    return results

def predict_single_pair_enhanced(text1, text2, vectorizer, scaler, model):
    """å˜ä¸€ãƒšã‚¢ã®äºˆæ¸¬ç¢ºç‡ã‚’è¨ˆç®—ã™ã‚‹æ”¹è‰¯ç‰ˆé–¢æ•°"""
    try:
        combined_text, features = create_enhanced_features(text1, text2)
        tfidf = vectorizer.transform([combined_text])
        features_scaled = scaler.transform(np.array([features]))
        X_test = hstack([tfidf, features_scaled])
        
        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ç¢ºç‡
        proba = model.predict_proba(X_test)[0][1]
        return proba
    except Exception as e:
        print(f"âš ï¸  äºˆæ¸¬ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
        return 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆç¢ºç‡

def save_results(results):
    """çµæœã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹é–¢æ•°"""
    print("\n=== çµæœã®ä¿å­˜ ===")
    submission_df = pd.DataFrame(results, columns=["id", "real_text_id"])
    print(f"ğŸ“Š äºˆæ¸¬çµæœã®åˆ†å¸ƒ:")
    print(submission_df["real_text_id"].value_counts())
    
    # çµæœã®ä¿å­˜
    submission_df.to_csv("submission_enhanced.csv", index=False)
    print("âœ… submission_enhanced.csvã‚’ä¿å­˜ã—ã¾ã—ãŸ")

def main():
    """ãƒ¡ã‚¤ãƒ³ã®å®Ÿè¡Œé–¢æ•°"""
    print("=== æ”¹è‰¯ç‰ˆãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã‚·ã‚¹ãƒ†ãƒ ã®é–‹å§‹ ===")
    
    # ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
    train_df = pd.read_csv("train.csv")
    print(f"ğŸ“Š è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®å½¢çŠ¶: {train_df.shape}")
    print(f"ğŸ“Š æ­£è§£ãƒ©ãƒ™ãƒ«ã®åˆ†å¸ƒ:")
    print(train_df['real_text_id'].value_counts())
    
    # æ‹¡å¼µç‰¹å¾´é‡ã®ä½œæˆ
    X_text, X_features, y = prepare_training_data(train_df)
    vectorizer, X_tfidf = create_optimized_tfidf_features(X_text)
    scaler, X_features_scaled = prepare_statistical_features(X_features)
    X_combined = combine_features(X_tfidf, X_features_scaled)
    
    # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã¨æœ€é©åŒ–
    model = train_and_optimize_model(X_combined, y)
    cv_scores = evaluate_model(model, X_combined, y)
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®äºˆæ¸¬ã¨çµæœã®ä¿å­˜
    results = predict_test_data_enhanced("test", vectorizer, scaler, model)
    save_results(results)
    
    print("\nğŸ‰ æ”¹è‰¯ç‰ˆã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè¡ŒãŒå®Œäº†ã—ã¾ã—ãŸï¼")

if __name__ == "__main__":
    main()