import os
import pandas as pd
import numpy as np
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report
from scipy.sparse import hstack

# === ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–¢é€£ã®é–¢æ•° ===
def read_text_pair(folder, article_index):
    """æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰file_1.txtã¨file_2.txtã‚’èª­ã¿è¾¼ã‚€é–¢æ•°"""
    folder_name = f"article_{int(article_index):04d}"
    base_path = os.path.join(folder, folder_name)
    with open(os.path.join(base_path, "file_1.txt"), encoding="utf-8") as f:
        file1 = f.read()
    with open(os.path.join(base_path, "file_2.txt"), encoding="utf-8") as f:
        file2 = f.read()
    return file1, file2

def extract_features(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰çµ±è¨ˆçš„ç‰¹å¾´é‡ã‚’æŠ½å‡ºã™ã‚‹é–¢æ•°"""
    # ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†ï¼ˆç©ºç™½ã®é™¤åŽ»ï¼‰
    text = text.strip()
    if not text:
        return [0] * 7  # ç©ºã®ãƒ†ã‚­ã‚¹ãƒˆã®å ´åˆã¯ã‚¼ãƒ­ã‚’è¿”ã™

    # åŸºæœ¬çµ±è¨ˆã‚’è¨ˆç®—
    words = text.split()  # å˜èªžã«åˆ†å‰²
    word_count = len(words)  # å˜èªžæ•°
    char_count = len(text)  # æ–‡å­—æ•°
    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]  # æ–‡ã«åˆ†å‰²
    sentence_count = len(sentences)  # æ–‡ã®æ•°

    # å¹³å‡æ–‡å­—é•·ã‚’è¨ˆç®—ï¼ˆNaNå›žé¿ã®ãŸã‚æ¡ä»¶åˆ†å²ï¼‰
    if words:
        avg_word_length = np.mean([len(word) for word in words])  # å¹³å‡å˜èªžé•·
    else:
        avg_word_length = 0

    # å¹³å‡æ–‡é•·ã‚’è¨ˆç®—
    if sentence_count > 0:
        avg_sentence_length = word_count / sentence_count  # 1æ–‡ã‚ãŸã‚Šã®å¹³å‡å˜èªžæ•°
    else:
        avg_sentence_length = 0

    # èªžå½™ã®å¤šæ§˜æ€§ã‚’è¨ˆç®—ï¼ˆåŒã˜å˜èªžã‚’ç¹°ã‚Šè¿”ã•ãªã„åº¦åˆã„ï¼‰
    if word_count > 0:
        unique_words = len(set(word.lower() for word in words))  # ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå˜èªžæ•°
        lexical_diversity = unique_words / word_count  # èªžå½™ã®å¤šæ§˜æ€§
    else:
        unique_words = 0
        lexical_diversity = 0

    # å¥èª­ç‚¹ã®å¯†åº¦ã‚’è¨ˆç®—
    if char_count > 0:
        punctuation_density = len(re.findall(r'[.!?,:;]', text)) / char_count
    else:
        punctuation_density = 0

    return [
        word_count, char_count, sentence_count,
        avg_word_length, avg_sentence_length,
        lexical_diversity, punctuation_density
    ]

def create_enhanced_features(file1, file2):
    """2ã¤ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ‹¡å¼µã•ã‚ŒãŸç‰¹å¾´é‡ã‚’ä½œæˆã™ã‚‹é–¢æ•°"""
    # TF-IDFç”¨ã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆ2ã¤ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’[SEP]ã§çµåˆï¼‰
    combined_text = file1 + " [SEP] " + file2

    # å„ãƒ•ã‚¡ã‚¤ãƒ«ã®çµ±è¨ˆçš„ç‰¹å¾´é‡ã‚’å–å¾—
    features1 = extract_features(file1)  # file1ã®çµ±è¨ˆ
    features2 = extract_features(file2)  # file2ã®çµ±è¨ˆ

    # å·®åˆ†ç‰¹å¾´é‡ï¼ˆfile1 - file2ï¼‰ã‚’è¨ˆç®—
    diff_features = [f1 - f2 for f1, f2 in zip(features1, features2)]

    # æ¯”çŽ‡ç‰¹å¾´é‡ï¼ˆfile1 / file2ï¼‰ã‚’è¨ˆç®—ï¼ˆ0é™¤ç®—å›žé¿ï¼‰
    ratio_features = []
    for f1, f2 in zip(features1, features2):
        if f2 != 0:
            ratio_features.append(f1 / f2)  # é€šå¸¸ã®æ¯”çŽ‡
        else:
            ratio_features.append(0 if f1 == 0 else 1)  # 0é™¤ç®—ã®å ´åˆã®å‡¦ç†

    # å…¨ã¦ã®ç‰¹å¾´é‡ã‚’çµåˆã—ã€NaNå€¤ã‚’0ã«ç½®æ›
    all_features = features1 + features2 + diff_features + ratio_features
    all_features = [0 if np.isnan(f) or np.isinf(f) else f for f in all_features]

    return combined_text, all_features


# === ç‰¹å¾´é‡ä½œæˆé–¢é€£ã®é–¢æ•° ===
def prepare_training_data(train_df):
    """è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç‰¹å¾´é‡ã¨ãƒ©ãƒ™ãƒ«ã‚’ä½œæˆã™ã‚‹é–¢æ•°"""
    X_text = []      # TF-IDFç”¨ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
    X_features = []  # çµ±è¨ˆçš„ç‰¹å¾´é‡
    y = []           # æ­£è§£ãƒ©ãƒ™ãƒ«

    print("\nå„è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç‰¹å¾´é‡ã‚’æŠ½å‡ºä¸­...")
    for _, row in train_df.iterrows():
        # å¯¾å¿œã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãƒšã‚¢ã‚’èª­ã¿è¾¼ã¿
        file1, file2 = read_text_pair("train", row["id"])

        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: file1 + [SEP] + file2 ã®é †åº
        combined_text1, features1 = create_enhanced_features(file1, file2)
        X_text.append(combined_text1)
        X_features.append(features1)
        y.append(1 if row["real_text_id"] == 1 else 0)

        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: file2 + [SEP] + file1 ã®é †åº
        combined_text2, features2 = create_enhanced_features(file2, file1)
        X_text.append(combined_text2)
        X_features.append(features2)
        y.append(1 if row["real_text_id"] == 2 else 0)

    print(f"ðŸ“Š å­¦ç¿’ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(X_text)} (å…ƒãƒ‡ãƒ¼ã‚¿ã®2å€)")
    print(f"ðŸ“Š ãƒ©ãƒ™ãƒ«åˆ†å¸ƒ: {pd.Series(y).value_counts()}")
    
    return X_text, X_features, y

def create_tfidf_features(X_text):
    """TF-IDFç‰¹å¾´é‡ã‚’ä½œæˆã™ã‚‹é–¢æ•°"""
    print("\n=== TF-IDFç‰¹å¾´é‡ã®ä½œæˆ ===")
    vectorizer = TfidfVectorizer(
        max_features=2000,
        min_df=2,
        max_df=0.8,
        ngram_range=(1, 2),
        stop_words='english'
    )
    X_tfidf = vectorizer.fit_transform(X_text)
    print(f"ðŸ“Š TF-IDFç‰¹å¾´é‡ã®å½¢çŠ¶: {X_tfidf.shape}")
    return vectorizer, X_tfidf

def prepare_statistical_features(X_features):
    """çµ±è¨ˆçš„ç‰¹å¾´é‡ã‚’æº–å‚™ã™ã‚‹é–¢æ•°"""
    print("\n=== çµ±è¨ˆçš„ç‰¹å¾´é‡ã®æº–å‚™ ===")
    X_features_array = np.array(X_features)
    
    if np.any(np.isnan(X_features_array)):
        print("âš ï¸  NaNå€¤ã‚’æ¤œå‡ºã—ã¾ã—ãŸã€‚0ã«ç½®æ›ã—ã¾ã™ã€‚")
        X_features_array = np.nan_to_num(X_features_array, nan=0.0)
    
    scaler = StandardScaler()
    X_features_scaled = scaler.fit_transform(X_features_array)
    print("âœ… çµ±è¨ˆçš„ç‰¹å¾´é‡ã‚’æ­£è¦åŒ–ã—ã¾ã—ãŸ")
    return scaler, X_features_scaled

def combine_features(X_tfidf, X_features_scaled):
    """ç‰¹å¾´é‡ã‚’çµåˆã™ã‚‹é–¢æ•°"""
    print("\n=== ç‰¹å¾´é‡ã®çµåˆ ===")
    X_combined = hstack([X_tfidf, X_features_scaled])
    
    if hasattr(X_combined, 'data') and np.any(np.isnan(X_combined.data)):
        print("âš ï¸  ã‚¹ãƒ‘ãƒ¼ã‚¹è¡Œåˆ—å†…ã®NaNå€¤ã‚’ä¿®æ­£ä¸­...")
        X_combined = X_combined.toarray()
        X_combined = np.nan_to_num(X_combined, nan=0.0)
    
    print(f"ðŸ“Š æœ€çµ‚çš„ãªç‰¹å¾´é‡è¡Œåˆ—: {X_combined.shape}")
    return X_combined

# === ãƒ¢ãƒ‡ãƒ«é–¢é€£ã®é–¢æ•° ===
def train_model(X_combined, y):
    """ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã™ã‚‹é–¢æ•°"""
    print("\n=== ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ ===")
    model = LogisticRegression(
        random_state=42,
        max_iter=5000,
        solver='liblinear',
        C=1.0,
        penalty='l2'
    )
    model.fit(X_combined, y)
    print("âœ… ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãŒå®Œäº†ã—ã¾ã—ãŸ")
    return model

def evaluate_model(model, X_combined, y):
    """ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’è©•ä¾¡ã™ã‚‹é–¢æ•°"""
    print("\n=== äº¤å·®æ¤œè¨¼ã«ã‚ˆã‚‹æ€§èƒ½è©•ä¾¡ ===")
    cv_scores = cross_val_score(model, X_combined, y, cv=5)
    print(f"ðŸ“Š äº¤å·®æ¤œè¨¼ã‚¹ã‚³ã‚¢: {cv_scores}")
    print(f"ðŸ“Š å¹³å‡ã‚¹ã‚³ã‚¢: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")
    return cv_scores

def predict_test_data(test_dir, vectorizer, scaler, model):
    """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®äºˆæ¸¬ã‚’è¡Œã†é–¢æ•°"""
    print("\n=== ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®äºˆæ¸¬ ===")
    test_folders = sorted([f for f in os.listdir(test_dir) if f.startswith("article_")])
    print(f"ðŸ“Š ãƒ†ã‚¹ãƒˆãƒ•ã‚©ãƒ«ãƒ€æ•°: {len(test_folders)}")
    
    results = []
    for i, folder in enumerate(test_folders):
        article_id = folder.split("_")[1]
        file1, file2 = read_text_pair(test_dir, article_id)
        
        # ä¸¡æ–¹ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ç‰¹å¾´é‡ã‚’ä½œæˆã¨äºˆæ¸¬
        prob1 = predict_single_pair(file1, file2, vectorizer, scaler, model)
        prob2 = predict_single_pair(file2, file1, vectorizer, scaler, model)
        
        # ã‚ˆã‚Šé«˜ã„ç¢ºçŽ‡ã‚’æŒã¤æ–¹ã‚’é¸æŠž
        real = 1 if prob1 > prob2 else 2
        results.append((int(article_id), real))
        
        if i < 5:
            print(f"ãƒ†ã‚¹ãƒˆ {i}: file1ç¢ºçŽ‡={prob1:.4f}, file2ç¢ºçŽ‡={prob2:.4f}, äºˆæ¸¬={real}")
    
    return results

def predict_single_pair(text1, text2, vectorizer, scaler, model):
    """å˜ä¸€ã®ãƒ†ã‚­ã‚¹ãƒˆãƒšã‚¢ã®äºˆæ¸¬ç¢ºçŽ‡ã‚’è¨ˆç®—ã™ã‚‹é–¢æ•°"""
    combined_text, features = create_enhanced_features(text1, text2)
    tfidf = vectorizer.transform([combined_text])
    features_scaled = scaler.transform(np.array([features]))
    X_test = hstack([tfidf, features_scaled])
    return model.predict_proba(X_test)[0][1]

def save_results(results):
    """çµæžœã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹é–¢æ•°"""
    print("\n=== çµæžœã®ä¿å­˜ ===")
    submission_df = pd.DataFrame(results, columns=["id", "real_text_id"])
    print(f"ðŸ“Š äºˆæ¸¬çµæžœã®åˆ†å¸ƒ:")
    print(submission_df["real_text_id"].value_counts())
    submission_df.to_csv("submission.csv", index=False)
    print("âœ… submission.csvã‚’ä¿å­˜ã—ã¾ã—ãŸ")

def main():
    """ãƒ¡ã‚¤ãƒ³ã®å®Ÿè¡Œé–¢æ•°"""
    # ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
    print("=== å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ã‚’é–‹å§‹ ===")
    train_df = pd.read_csv("train.csv")
    print(f"ðŸ“Š è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®å½¢çŠ¶: {train_df.shape}")
    print(f"ðŸ“Š æ­£è§£ãƒ©ãƒ™ãƒ«ã®åˆ†å¸ƒ:")
    print(train_df['real_text_id'].value_counts())

    # ç‰¹å¾´é‡ã®ä½œæˆ
    X_text, X_features, y = prepare_training_data(train_df)
    vectorizer, X_tfidf = create_tfidf_features(X_text)
    scaler, X_features_scaled = prepare_statistical_features(X_features)
    X_combined = combine_features(X_tfidf, X_features_scaled)

    # ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã¨è©•ä¾¡
    model = train_model(X_combined, y)
    cv_scores = evaluate_model(model, X_combined, y)

    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®äºˆæ¸¬ã¨çµæžœã®ä¿å­˜
    results = predict_test_data("test", vectorizer, scaler, model)
    save_results(results)

if __name__ == "__main__":
    main()