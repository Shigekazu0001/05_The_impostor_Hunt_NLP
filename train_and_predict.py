import os
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report
import re

def read_text_pair(folder, article_index):
    """
    æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰file_1.txtã¨file_2.txtã‚’èª­ã¿è¾¼ã‚€é–¢æ•°
    """
    folder_name = f"article_{int(article_index):04d}"
    base_path = os.path.join(folder, folder_name)
    with open(os.path.join(base_path, "file_1.txt"), encoding="utf-8") as f:
        file1 = f.read()
    with open(os.path.join(base_path, "file_2.txt"), encoding="utf-8") as f:
        file2 = f.read()
    return file1, file2

def extract_features(text):
    """
    ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰çµ±è¨ˆçš„ç‰¹å¾´é‡ã‚’æŠ½å‡ºã™ã‚‹é–¢æ•°
    äººé–“ã¨AIã®æ–‡ç« ã®é•ã„ã‚’æ•°å€¤åŒ–ã—ã¦æ‰ãˆã‚‹ãŸã‚ã«ä½¿ç”¨
    """
    # ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†ï¼ˆç©ºç™½ã®é™¤å»ï¼‰
    text = text.strip()
    if not text:
        return [0] * 7  # ç©ºã®ãƒ†ã‚­ã‚¹ãƒˆã®å ´åˆã¯ã‚¼ãƒ­ã‚’è¿”ã™
    
    # åŸºæœ¬çµ±è¨ˆã‚’è¨ˆç®—
    words = text.split()  # å˜èªã«åˆ†å‰²
    word_count = len(words)  # å˜èªæ•°
    char_count = len(text)  # æ–‡å­—æ•°
    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]  # æ–‡ã«åˆ†å‰²
    sentence_count = len(sentences)  # æ–‡ã®æ•°
    
    # å¹³å‡æ–‡å­—é•·ã‚’è¨ˆç®—ï¼ˆNaNå›é¿ã®ãŸã‚æ¡ä»¶åˆ†å²ï¼‰
    if words:
        avg_word_length = np.mean([len(word) for word in words])  # å¹³å‡å˜èªé•·
    else:
        avg_word_length = 0
    
    # å¹³å‡æ–‡é•·ã‚’è¨ˆç®—
    if sentence_count > 0:
        avg_sentence_length = word_count / sentence_count  # 1æ–‡ã‚ãŸã‚Šã®å¹³å‡å˜èªæ•°
    else:
        avg_sentence_length = 0
    
    # èªå½™ã®å¤šæ§˜æ€§ã‚’è¨ˆç®—ï¼ˆåŒã˜å˜èªã‚’ç¹°ã‚Šè¿”ã•ãªã„åº¦åˆã„ï¼‰
    if word_count > 0:
        unique_words = len(set(word.lower() for word in words))  # ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå˜èªæ•°
        lexical_diversity = unique_words / word_count  # èªå½™ã®å¤šæ§˜æ€§
    else:
        unique_words = 0
        lexical_diversity = 0
    
    # å¥èª­ç‚¹ã®å¯†åº¦ã‚’è¨ˆç®—
    if char_count > 0:
        punctuation_density = len(re.findall(r'[.!?,:;]', text)) / char_count
    else:
        punctuation_density = 0
    
    return [
        word_count, char_count, sentence_count,
        avg_word_length, avg_sentence_length,
        lexical_diversity, punctuation_density
    ]

def create_enhanced_features(file1, file2):
    """
    2ã¤ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ‹¡å¼µã•ã‚ŒãŸç‰¹å¾´é‡ã‚’ä½œæˆã™ã‚‹é–¢æ•°
    ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ï¼ˆTF-IDFç”¨ï¼‰ã¨çµ±è¨ˆçš„ç‰¹å¾´é‡ã‚’ä¸¡æ–¹ä½œæˆ
    """
    # TF-IDFç”¨ã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆ2ã¤ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’[SEP]ã§çµåˆï¼‰
    combined_text = file1 + " [SEP] " + file2
    
    # å„ãƒ•ã‚¡ã‚¤ãƒ«ã®çµ±è¨ˆçš„ç‰¹å¾´é‡ã‚’å–å¾—
    features1 = extract_features(file1)  # file1ã®çµ±è¨ˆ
    features2 = extract_features(file2)  # file2ã®çµ±è¨ˆ
    
    # å·®åˆ†ç‰¹å¾´é‡ï¼ˆfile1 - file2ï¼‰ã‚’è¨ˆç®—
    # ä¾‹ï¼šfile1ã®å˜èªæ•° - file2ã®å˜èªæ•°
    diff_features = [f1 - f2 for f1, f2 in zip(features1, features2)]
    
    # æ¯”ç‡ç‰¹å¾´é‡ï¼ˆfile1 / file2ï¼‰ã‚’è¨ˆç®—ï¼ˆ0é™¤ç®—å›é¿ï¼‰
    ratio_features = []
    for f1, f2 in zip(features1, features2):
        if f2 != 0:
            ratio_features.append(f1 / f2)  # é€šå¸¸ã®æ¯”ç‡
        else:
            ratio_features.append(0 if f1 == 0 else 1)  # 0é™¤ç®—ã®å ´åˆã®å‡¦ç†
    
    # å…¨ã¦ã®ç‰¹å¾´é‡ã‚’çµåˆã—ã€NaNå€¤ã‚’0ã«ç½®æ›
    all_features = features1 + features2 + diff_features + ratio_features
    all_features = [0 if np.isnan(f) or np.isinf(f) else f for f in all_features]
    
    return combined_text, all_features

# === å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ ===
print("=== å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ã‚’é–‹å§‹ ===")

# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
train_df = pd.read_csv("train.csv")
print(f"ğŸ“Š è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®å½¢çŠ¶: {train_df.shape}")
print(f"ğŸ“Š æ­£è§£ãƒ©ãƒ™ãƒ«ã®åˆ†å¸ƒ:")
print(train_df['real_text_id'].value_counts())

# å­¦ç¿’ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã¨ãƒ©ãƒ™ãƒ«ã‚’æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆ
X_text = []  # TF-IDFç”¨ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
X_features = []  # çµ±è¨ˆçš„ç‰¹å¾´é‡
y = []  # æ­£è§£ãƒ©ãƒ™ãƒ«

print("\nå„è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç‰¹å¾´é‡ã‚’æŠ½å‡ºä¸­...")
for idx, row in train_df.iterrows():
    # å¯¾å¿œã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãƒšã‚¢ã‚’èª­ã¿è¾¼ã¿
    file1, file2 = read_text_pair("train", row["id"])
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³1: file1 + [SEP] + file2 ã®é †åº
    # ã“ã®å ´åˆã€file1ãŒæœ¬ç‰©ã‹ã©ã†ã‹ã‚’å­¦ç¿’
    combined_text1, features1 = create_enhanced_features(file1, file2)
    X_text.append(combined_text1)
    X_features.append(features1)
    y.append(1 if row["real_text_id"] == 1 else 0)  # file1ãŒæœ¬ç‰©ãªã‚‰1ã€ãã†ã§ãªã‘ã‚Œã°0
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³2: file2 + [SEP] + file1 ã®é †åº
    # ã“ã®å ´åˆã€file2ãŒæœ¬ç‰©ã‹ã©ã†ã‹ã‚’å­¦ç¿’
    combined_text2, features2 = create_enhanced_features(file2, file1)
    X_text.append(combined_text2)
    X_features.append(features2)
    y.append(1 if row["real_text_id"] == 2 else 0)  # file2ãŒæœ¬ç‰©ãªã‚‰1ã€ãã†ã§ãªã‘ã‚Œã°0

print(f"ğŸ“Š å­¦ç¿’ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(X_text)} (å…ƒãƒ‡ãƒ¼ã‚¿ã®2å€)")
print(f"ğŸ“Š ãƒ©ãƒ™ãƒ«åˆ†å¸ƒ: {pd.Series(y).value_counts()}")

# === TF-IDFç‰¹å¾´é‡ã®ä½œæˆ ===
print("\n=== TF-IDFç‰¹å¾´é‡ã®ä½œæˆ ===")

# TF-IDFãƒ™ã‚¯ãƒˆãƒ©ã‚¤ã‚¶ãƒ¼ã‚’è¨­å®šï¼ˆç‰¹å¾´é‡æ•°ã‚’å‰Šæ¸›ï¼‰
vectorizer = TfidfVectorizer(
    max_features=2000,      # ç‰¹å¾´é‡æ•°ã‚’5000ã‹ã‚‰2000ã«å‰Šæ¸›
    min_df=2,               # æœ€ä½2å›ä»¥ä¸Šå‡ºç¾ã™ã‚‹å˜èªã®ã¿ä½¿ç”¨
    max_df=0.8,             # 80%ä»¥ä¸Šã®æ–‡æ›¸ã«å‡ºç¾ã™ã‚‹å˜èªã¯é™¤å¤–
    ngram_range=(1, 2),     # 1-gramï¼ˆå˜èªï¼‰ã¨2-gramï¼ˆå˜èªãƒšã‚¢ï¼‰ã‚’ä½¿ç”¨
    stop_words='english'    # è‹±èªã®ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ï¼ˆthe, a, anãªã©ï¼‰ã‚’é™¤å»
)

# ãƒ†ã‚­ã‚¹ãƒˆã‚’TF-IDFãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›
X_tfidf = vectorizer.fit_transform(X_text)
print(f"ğŸ“Š TF-IDFç‰¹å¾´é‡ã®å½¢çŠ¶: {X_tfidf.shape}")

# === çµ±è¨ˆçš„ç‰¹å¾´é‡ã®æº–å‚™ ===
print("\n=== çµ±è¨ˆçš„ç‰¹å¾´é‡ã®æº–å‚™ ===")

# çµ±è¨ˆçš„ç‰¹å¾´é‡ã‚’é…åˆ—ã«å¤‰æ›
X_features_array = np.array(X_features)
print(f"ğŸ“Š çµ±è¨ˆçš„ç‰¹å¾´é‡ã®å½¢çŠ¶: {X_features_array.shape}")

# NaNå€¤ã®ãƒã‚§ãƒƒã‚¯ã¨ä¿®æ­£
if np.any(np.isnan(X_features_array)):
    print("âš ï¸  NaNå€¤ã‚’æ¤œå‡ºã—ã¾ã—ãŸã€‚0ã«ç½®æ›ã—ã¾ã™ã€‚")
    X_features_array = np.nan_to_num(X_features_array, nan=0.0)

# çµ±è¨ˆçš„ç‰¹å¾´é‡ã®æ­£è¦åŒ–
scaler = StandardScaler()
X_features_scaled = scaler.fit_transform(X_features_array)
print("âœ… çµ±è¨ˆçš„ç‰¹å¾´é‡ã‚’æ­£è¦åŒ–ã—ã¾ã—ãŸ")

# === ç‰¹å¾´é‡ã®çµåˆ ===
print("\n=== ç‰¹å¾´é‡ã®çµåˆ ===")

# TF-IDFç‰¹å¾´é‡ã¨æ­£è¦åŒ–ã•ã‚ŒãŸçµ±è¨ˆçš„ç‰¹å¾´é‡ã‚’çµåˆ
from scipy.sparse import hstack
X_combined = hstack([X_tfidf, X_features_scaled])
print(f"ğŸ“Š çµåˆå¾Œã®ç‰¹å¾´é‡ã®å½¢çŠ¶: {X_combined.shape}")

# ã‚¹ãƒ‘ãƒ¼ã‚¹è¡Œåˆ—å†…ã®NaNå€¤ãƒã‚§ãƒƒã‚¯
if hasattr(X_combined, 'data') and np.any(np.isnan(X_combined.data)):
    print("âš ï¸  ã‚¹ãƒ‘ãƒ¼ã‚¹è¡Œåˆ—å†…ã®NaNå€¤ã‚’ä¿®æ­£ä¸­...")
    X_combined = X_combined.toarray()
    X_combined = np.nan_to_num(X_combined, nan=0.0)
    
print(f"ğŸ“Š æœ€çµ‚çš„ãªç‰¹å¾´é‡è¡Œåˆ—: {X_combined.shape}")

# === ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ ===
print("\n=== ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ ===")

# ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆï¼ˆåæŸå•é¡Œã‚’è§£æ±ºï¼‰
model = LogisticRegression(
    random_state=42, 
    max_iter=5000,          # åå¾©å›æ•°ã‚’å¢—åŠ 
    solver='liblinear',     # å°ã•ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«é©ã—ãŸã‚½ãƒ«ãƒãƒ¼
    C=1.0,                  # æ­£å‰‡åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    penalty='l2'            # L2æ­£å‰‡åŒ–
)
model.fit(X_combined, y)
print("âœ… ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãŒå®Œäº†ã—ã¾ã—ãŸ")

# === äº¤å·®æ¤œè¨¼ã«ã‚ˆã‚‹æ€§èƒ½è©•ä¾¡ ===
print("\n=== äº¤å·®æ¤œè¨¼ã«ã‚ˆã‚‹æ€§èƒ½è©•ä¾¡ ===")

# 5åˆ†å‰²äº¤å·®æ¤œè¨¼ã§ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’è©•ä¾¡
cv_scores = cross_val_score(model, X_combined, y, cv=5)
print(f"ğŸ“Š äº¤å·®æ¤œè¨¼ã‚¹ã‚³ã‚¢: {cv_scores}")
print(f"ğŸ“Š å¹³å‡ã‚¹ã‚³ã‚¢: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")

# === ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®äºˆæ¸¬ ===
print("\n=== ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®äºˆæ¸¬ ===")

test_dir = "test"
test_folders = sorted([f for f in os.listdir(test_dir) if f.startswith("article_")])
print(f"ğŸ“Š ãƒ†ã‚¹ãƒˆãƒ•ã‚©ãƒ«ãƒ€æ•°: {len(test_folders)}")

results = []

print("å„ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’äºˆæ¸¬ä¸­...")
for i, folder in enumerate(test_folders):
    # ãƒ•ã‚©ãƒ«ãƒ€åã‹ã‚‰IDã‚’æŠ½å‡º
    article_id = folder.split("_")[1]
    
    # ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
    file1, file2 = read_text_pair(test_dir, article_id)
    
    # ä¸¡æ–¹ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ç‰¹å¾´é‡ã‚’ä½œæˆ
    combined_text1, features1 = create_enhanced_features(file1, file2)  # file1ãŒæœ€åˆ
    combined_text2, features2 = create_enhanced_features(file2, file1)  # file2ãŒæœ€åˆ
    
    # TF-IDFå¤‰æ›
    tfidf1 = vectorizer.transform([combined_text1])
    tfidf2 = vectorizer.transform([combined_text2])
    
    # çµ±è¨ˆçš„ç‰¹å¾´é‡ã®æ­£è¦åŒ–
    features1_scaled = scaler.transform(np.array([features1]))
    features2_scaled = scaler.transform(np.array([features2]))
    
    # ç‰¹å¾´é‡ã‚’çµåˆ
    X_test1 = hstack([tfidf1, features1_scaled])
    X_test2 = hstack([tfidf2, features2_scaled])
    
    # äºˆæ¸¬ç¢ºç‡ã‚’è¨ˆç®—
    prob1 = model.predict_proba(X_test1)[0][1]  # file1ãŒæœ¬ç‰©ã§ã‚ã‚‹ç¢ºç‡
    prob2 = model.predict_proba(X_test2)[0][1]  # file2ãŒæœ¬ç‰©ã§ã‚ã‚‹ç¢ºç‡
    
    # ã‚ˆã‚Šé«˜ã„ç¢ºç‡ã‚’æŒã¤æ–¹ã‚’é¸æŠ
    real = 1 if prob1 > prob2 else 2
    # IDã‚’æ•°å€¤ã«å¤‰æ›ã—ã¦ä¿å­˜
    results.append((int(article_id), real))
    
    # æœ€åˆã®5å€‹ã¯è©³ç´°ã‚’è¡¨ç¤º
    if i < 5:
        print(f"ãƒ†ã‚¹ãƒˆ {i}: file1ç¢ºç‡={prob1:.4f}, file2ç¢ºç‡={prob2:.4f}, äºˆæ¸¬={real}")

# === çµæœã®ä¿å­˜ ===
print("\n=== çµæœã®ä¿å­˜ ===")

submission_df = pd.DataFrame(results, columns=["id", "real_text_id"])
print(f"ğŸ“Š äºˆæ¸¬çµæœã®åˆ†å¸ƒ:")
print(submission_df["real_text_id"].value_counts())

submission_df.to_csv("submission.csv", index=False)
print("âœ… submission.csvã‚’ä¿å­˜ã—ã¾ã—ãŸ")