# テキスト分類システム README

## 概要
このシステムは、2つのテキストファイルのペアを比較し、どちらが「本物」のテキストかを機械学習によって判定するシステムです。TF-IDF特徴量と統計的特徴量を組み合わせ、アンサンブル学習により高精度な分類を実現しています。

## システムの全体構成

### 1. データ構造
- **訓練データ**: `train/article_XXXX/file_1.txt`, `file_2.txt`
- **テストデータ**: `test/article_XXXX/file_1.txt`, `file_2.txt`
- **ラベル**: `train.csv` (どちらが本物のテキストかを示す)

### 2. 特徴量抽出の流れ

#### 2.1 基本統計的特徴量 (16次元)
各テキストから以下の特徴量を抽出：
- **基本統計**: 単語数、文字数、文数、平均単語長、平均文長
- **語彙特徴**: 語彙の多様性、句読点密度、感嘆符・疑問符の数
- **品質特徴**: スペルエラー数、文法エラー数
- **内容特徴**: 専門用語密度、感情スコア、主観性スコア
- **構造特徴**: 文の複雑性、大文字使用率

#### 2.2 拡張特徴量の作成
- **個別特徴量**: file1用16次元 + file2用16次元 = 32次元
- **差分特徴量**: file1 - file2 = 16次元
- **比率特徴量**: file1 / file2 = 16次元
- **比較特徴量**: 語彙重複度、文長分散差 = 2次元
- **合計**: 66次元の統計的特徴量

#### 2.3 TF-IDF特徴量
- **結合テキスト**: "file1 [SEP] file2"形式で結合
- **パラメータ**: max_features=3000, ngram_range=(1,3), min_df=2, max_df=0.85
- **出力**: 3000次元のTF-IDF特徴量

### 3. データ拡張戦略
各訓練サンプルから2つのパターンを生成：
1. **パターン1**: file1 + file2の組み合わせ → ラベル: file1が本物なら1, そうでなければ0
2. **パターン2**: file2 + file1の組み合わせ → ラベル: file2が本物なら1, そうでなければ0

これにより訓練データを2倍に拡張し、モデルの汎化性能を向上。

### 4. 機械学習モデル

#### 4.1 アンサンブル構成
- **ベースモデル1**: ロジスティック回帰 (線形分類器)
- **ベースモデル2**: ランダムフォレスト (非線形分類器)
- **統合方法**: ソフト投票（確率による重み付け平均）

#### 4.2 ハイパーパラメータ最適化
グリッドサーチにより以下のパラメータを最適化：
- **ロジスティック回帰**: C値 (正則化強度)
- **ランダムフォレスト**: n_estimators (木の数), max_depth (木の深さ)

### 5. 予測フロー

#### 5.1 テストデータの処理
各テストサンプルに対して：
1. file1 → file2、file2 → file1の2つの組み合わせで予測
2. 各組み合わせを3回予測し、平均を算出（予測の安定性向上）
3. 2つの確率を比較し、高い方を「本物」として判定

#### 5.2 信頼度の計算
予測信頼度 = |prob1 - prob2| として、予測の確実性を測定

## 実行フロー

### 1. データ準備段階
```python
# 訓練データの読み込み
train_df = pd.read_csv("train.csv")

# 各記事ペアから特徴量を抽出
X_text, X_features, y = prepare_training_data(train_df)
```

### 2. 特徴量作成段階
```python
# TF-IDF特徴量の作成
vectorizer, X_tfidf = create_optimized_tfidf_features(X_text)

# 統計的特徴量の正規化
scaler, X_features_scaled = prepare_statistical_features(X_features)

# 特徴量の結合
X_combined = combine_features(X_tfidf, X_features_scaled)
```

### 3. モデル学習段階
```python
# アンサンブルモデルの学習と最適化
model = train_and_optimize_model(X_combined, y)

# 交差検証による性能評価
cv_scores = evaluate_model(model, X_combined, y)
```

### 4. 予測段階
```python
# テストデータの予測
results = predict_test_data_enhanced("test", vectorizer, scaler, model)

# 結果の保存
save_results(results)
```

## 主要な技術的特徴

### 1. ロバストな特徴量設計
- **多角度分析**: 統計、品質、内容、構造の4つの観点から特徴量を抽出
- **エラー検出**: スペルミス、文法エラーの自動検出
- **専門性評価**: 技術用語密度による文書の専門性測定

### 2. 高度な前処理
- **欠損値処理**: NaN/Inf値の自動検出・修正
- **次元整合**: 特徴量次元の自動調整
- **正規化**: StandardScalerによる特徴量の標準化

### 3. 予測安定性の向上
- **複数回予測**: 各テストサンプルを3回予測し平均化
- **双方向評価**: 両方向の組み合わせで予測し比較
- **信頼度評価**: 予測の確実性を定量化

### 4. パフォーマンス最適化
- **並列処理**: n_jobs=-1による並列実行
- **効率的な特徴量**: スパース行列による メモリ効率化
- **バッチ処理**: 大量データの効率的な処理

## 出力ファイル

### submission_enhanced.csv
- **形式**: id, real_text_id
- **内容**: 各テスト記事に対する予測結果
- **値**: 1 (file_1.txtが本物) または 2 (file_2.txtが本物)

## 実行要件

### 必要なライブラリ
```bash
pip install pandas numpy scikit-learn scipy
```

### ディレクトリ構造
```
project/
├── train/
│   └── article_XXXX/
│       ├── file_1.txt
│       └── file_2.txt
├── test/
│   └── article_XXXX/
│       ├── file_1.txt
│       └── file_2.txt
├── train.csv
└── train_and_predict.py
```

## 使用方法

```bash
python train_and_predict.py
```

実行すると、自動的に訓練、最適化、予測、保存の全工程が実行され、`submission_enhanced.csv`が生成されます。

## 特徴量の詳細

### 統計的特徴量の内訳
1. **基本統計** (5次元): 単語数、文字数、文数、平均単語長、平均文長
2. **語彙分析** (3次元): 語彙多様性、句読点密度、記号使用頻度
3. **品質評価** (2次元): スペルエラー数、文法エラー数
4. **内容分析** (3次元): 専門用語密度、感情スコア、主観性スコア
5. **構造分析** (2次元): 文章複雑性、大文字使用率
6. **テキスト比較** (1次元): 語彙重複度、文長分散差

### TF-IDF特徴量の詳細
- **N-gram**: 1-gram, 2-gram, 3-gramの組み合わせ
- **正規化**: L2正規化による重み調整
- **フィルタリング**: 最頻出語と稀少語の除去
- **特徴選択**: 最大3000次元に制限

これらの特徴量を組み合わせることで、テキストの真偽を多角的に判定する高性能なシステムを実現しています。